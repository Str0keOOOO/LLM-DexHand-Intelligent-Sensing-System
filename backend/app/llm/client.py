from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from app.config import settings
import asyncio # å¼•å…¥ asyncio å¤„ç†è¶…æ—¶

def get_llm_model(model_name: str):
    """
    æ ¹æ®æ¨¡å‹åç§°è¿”å›å¯¹åº”çš„ LangChain æ¨¡å‹å®ä¾‹
    """
    model_name = model_name.lower()

    if "/" in model_name and not "all-minilm" in model_name:
        print(f"ğŸš€ Using SiliconFlow API: {model_name}")
        return ChatOpenAI(
            model=model_name,
            api_key=settings.SILICONFLOW_API_KEY,
            base_url=settings.SILICONFLOW_BASE_URL,
            temperature=0.7,
            timeout=10.0 # ç¡…åŸºæµåŠ¨é€Ÿåº¦å¾ˆå¿«ï¼Œ10ç§’è¶³å¤Ÿ
        )
    

def ask_ai(text: str, system_prompt: str, model_name: str = None) -> tuple[str, str]:
    """
    é€šç”¨ AI è°ƒç”¨å‡½æ•°
    Returns: (ai_reply_text, used_model_name)
    """
    # 1. ç¡®å®šè¦ä½¿ç”¨çš„æ¨¡å‹åç§°
    target_model = model_name if model_name else settings.DEFAULT_LLM_MODEL
    
    try:
        # 2. è·å– LangChain æ¨¡å‹å®ä¾‹
        llm = get_llm_model(target_model)
        print(f"ğŸ¤– Invoking model: {target_model}")
        
        # 3. æ„å»º Prompt
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("user", "{input}")
        ])

        # 4. æ„å»ºé“¾ (Chain)
        chain = prompt | llm | StrOutputParser()

        # 5. æ‰§è¡Œ
        response_text = chain.invoke({"input": text})
        
        return response_text, target_model

    except Exception as e:
        print(f"âŒ LLM Error ({target_model}): {e}")
        return f"æ¨¡å‹ {target_model} è°ƒç”¨å¤±è´¥: {str(e)}", target_model
    
async def validate_model(model_name: str) -> tuple[bool, str]:
    """
    å¼‚æ­¥éªŒè¯æ¨¡å‹è¿æ¥ï¼Œé˜²æ­¢é˜»å¡ä¸»çº¿ç¨‹ã€‚
    """
    try:
        # 1. è·å–æ¨¡å‹å®ä¾‹
        llm = get_llm_model(model_name)
        print(f"ğŸ” Validating model connection: {model_name}")
        print(f"LLM Details: {llm}")
        # 2. [å…³é”®ä¿®æ”¹] ä½¿ç”¨ ainvoke (å¼‚æ­¥) ä»£æ›¿ invoke (åŒæ­¥)
        # å¹¶å¢åŠ  timeout æ§åˆ¶ï¼Œé˜²æ­¢ä¸€ç›´å¡ç€
        try:
            # è®¾å®š 5 ç§’è¶…æ—¶ï¼Œè¿ä¸ä¸Šå°±ç«‹åˆ»è¿”å›å¤±è´¥ï¼Œä¸è¦å‚»ç­‰
            response = await asyncio.wait_for(llm.ainvoke("hi"), timeout=5.0)
        except asyncio.TimeoutError:
            return False, "Connection Timeout (5s)"

        # 3. æ£€æŸ¥ç»“æœ
        if response and response.content:
            return True, "Connected successfully"
        else:
            return False, "Empty response from model"
            
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Validation Error: {error_msg}")
        if "401" in error_msg:
             return False, "Auth Failed (Check API Key)"
        if "Connection refused" in error_msg:
            return False, "Connection Refused (Is Ollama running?)"
        return False, f"Error: {error_msg[:100]}..."