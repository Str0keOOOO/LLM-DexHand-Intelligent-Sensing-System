import os
import asyncio
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import json


AVAILABLE_MODELS = os.getenv("AVAILABLE_MODELS")
print(f"ğŸ” Loaded AVAILABLE_MODELS env: {AVAILABLE_MODELS}")
AVAILABLE_MODELS = json.loads(AVAILABLE_MODELS)


def get_llm_model(model_name: str):
    """
    æ ¹æ®æ¨¡å‹åç§°è¿”å›å¯¹åº”çš„ LangChain æ¨¡å‹å®ä¾‹
    """
    model_name = model_name.lower() if model_name else ""

    # --- æƒ…å†µ A: ç¡…åŸºæµåŠ¨ / DeepSeek (OpenAI åè®®) ---
    if "/" in model_name or "gpt" in model_name:
        print(f"ğŸš€ Using SiliconFlow/OpenAI API: {model_name}")
        return ChatOpenAI(
            model=model_name,
            api_key=os.getenv("SILICONFLOW_API_KEY") or os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1"),
            temperature=0.7,
            timeout=10.0,
            max_retries=2,
        )

    # --- æƒ…å†µ B: æœ¬åœ° Ollama ---
    else:
        print(f"ğŸ¦™ Using Local Ollama: {model_name or 'default'}")
        return ChatOllama(model=model_name if model_name else "llama3", base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"), temperature=0.7, timeout=30.0)


def ask_ai(text: str, system_prompt: str, model_name: str = None) -> tuple[str, str]:
    """
    é€šç”¨ AI è°ƒç”¨å‡½æ•°
    Returns: (ai_reply_text, used_model_name)
    """
    # [å…³é”®ä¿®æ”¹] 1. ç¡®å®šè¦ä½¿ç”¨çš„æ¨¡å‹åç§°
    # å¦‚æœæ²¡ä¼  model_nameï¼Œåˆ™é»˜è®¤ä½¿ç”¨åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
    if not model_name:
        if AVAILABLE_MODELS:
            default_model = AVAILABLE_MODELS[0]["value"]
        else:
            default_model = "gpt-3.5-turbo"  # æœ€åçš„ä¿åº•
    else:
        default_model = model_name

    target_model = default_model

    try:
        # 2. è·å– LangChain æ¨¡å‹å®ä¾‹
        llm = get_llm_model(target_model)
        print(f"ğŸ¤– Invoking model: {target_model}")

        # 3. æ„å»º Prompt
        prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("user", "{input}")])

        # 4. æ„å»ºé“¾ (Chain)
        chain = prompt | llm | StrOutputParser()

        # 5. æ‰§è¡Œ
        response_text = chain.invoke({"input": text})

        return response_text, target_model

    except Exception as e:
        print(f"âŒ LLM Error ({target_model}): {e}")
        return f"æ¨¡å‹ {target_model} è°ƒç”¨å¤±è´¥: {str(e)}", target_model


async def validate_model(model_name: str) -> tuple[bool, str]:
    """
    å¼‚æ­¥éªŒè¯æ¨¡å‹è¿æ¥
    """
    try:
        llm = get_llm_model(model_name)
        print(f"ğŸ” Validating model connection: {model_name}")

        try:
            chain = llm | StrOutputParser()
            # å‘é€ç®€å•çš„ hi æµ‹è¯•è¿æ¥
            response = await asyncio.wait_for(chain.ainvoke("hi"), timeout=5.0)
        except asyncio.TimeoutError:
            return False, "Connection Timeout (5s)"

        if response:
            return True, "Connected successfully"
        else:
            return False, "Empty response from model"

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Validation Error: {error_msg}")
        if "401" in error_msg:
            return False, "Auth Failed (Check API Key)"
        if "Connection refused" in error_msg:
            return False, "Connection Refused (Is Ollama running?)"
        return False, f"Error: {error_msg[:100]}..."
